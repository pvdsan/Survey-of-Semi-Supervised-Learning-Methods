{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./mnist_subset_combined.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanket\\AppData\\Local\\Temp\\ipykernel_24360\\2831219842.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  images = torch.tensor([item['image'] for item in data], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Convert the structured array to PyTorch tensors\n",
    "images = torch.tensor([item['image'] for item in data], dtype=torch.float32)\n",
    "labels = torch.tensor([item['label'] for item in data], dtype=torch.long)\n",
    "\n",
    "# Flatten the images for a simple fully connected network\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "\n",
    "# Create a dataset and data loader\n",
    "dataset = TensorDataset(images, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = {k: [] for k in range(10)}  # Assuming 10 classes (0-9)\n",
    "for idx, (image, label) in enumerate(dataset):\n",
    "    class_indices[label.item()].append(idx)\n",
    "\n",
    "# Step 2: Randomly select 10 samples from each class\n",
    "labeled_indices = []\n",
    "for indices in class_indices.values():\n",
    "    labeled_indices.extend(np.random.choice(indices, 10, replace=False))\n",
    "\n",
    "# Create a mask for the rest of the data for testing\n",
    "mask = np.ones(len(dataset), dtype=bool)\n",
    "mask[labeled_indices] = False\n",
    "unlabeled_indices = np.arange(len(dataset))[mask]\n",
    "\n",
    "# Step 3: Create training and testing subsets\n",
    "labeled_set = Subset(dataset, labeled_indices)\n",
    "unlabeled_set = Subset(dataset, unlabeled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create DataLoaders\n",
    "labeledLoader = DataLoader(labeled_set, batch_size=10, shuffle=True)\n",
    "unlabeledLoader = DataLoader(unlabeled_set, batch_size=50, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net,self).__init__()\n",
    "\n",
    "        self.fullyConnectedLayer = nn.Sequential(\n",
    "            nn.Linear(784, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200,10)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        output = self.fullyConnectedLayer(input)\n",
    "        activatedOutput = F.log_softmax(output, dim = 1)\n",
    "\n",
    "        return activatedOutput\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_label(model, unlabeled_loader, threshold=0.6):\n",
    "    model.eval()\n",
    "    pseudo_labeled = []\n",
    "    for images, _ in unlabeled_loader:\n",
    "        outputs = model(images)\n",
    "        prob, max_indices = outputs.max(dim=1)\n",
    "        mask = prob > threshold\n",
    "        masked_images = images[mask]\n",
    "        masked_indices = max_indices[mask]\n",
    "        pseudo_labeled.append((masked_images, masked_indices))\n",
    "        \n",
    "    return torch.cat([x[0] for x in pseudo_labeled]), torch.cat([x[1] for x in pseudo_labeled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The supervised loss for epoch0 is:2.3024277687072754\n",
      "The pseudo loss for epoch0 is:nan\n",
      "The supervised loss for epoch1 is:2.303675889968872\n",
      "The pseudo loss for epoch1 is:nan\n",
      "The supervised loss for epoch2 is:2.302732229232788\n",
      "The pseudo loss for epoch2 is:nan\n",
      "The supervised loss for epoch3 is:2.3059582710266113\n",
      "The pseudo loss for epoch3 is:nan\n",
      "The supervised loss for epoch4 is:2.306919574737549\n",
      "The pseudo loss for epoch4 is:nan\n",
      "The supervised loss for epoch5 is:2.3041348457336426\n",
      "The pseudo loss for epoch5 is:nan\n",
      "The supervised loss for epoch6 is:2.3087477684020996\n",
      "The pseudo loss for epoch6 is:nan\n",
      "The supervised loss for epoch7 is:2.304234266281128\n",
      "The pseudo loss for epoch7 is:nan\n",
      "The supervised loss for epoch8 is:2.3035664558410645\n",
      "The pseudo loss for epoch8 is:nan\n",
      "The supervised loss for epoch9 is:2.304051637649536\n",
      "The pseudo loss for epoch9 is:nan\n",
      "The supervised loss for epoch10 is:2.305117130279541\n",
      "The pseudo loss for epoch10 is:nan\n",
      "The supervised loss for epoch11 is:2.304342746734619\n",
      "The pseudo loss for epoch11 is:nan\n",
      "The supervised loss for epoch12 is:2.3057899475097656\n",
      "The pseudo loss for epoch12 is:nan\n",
      "The supervised loss for epoch13 is:2.3080451488494873\n",
      "The pseudo loss for epoch13 is:nan\n",
      "The supervised loss for epoch14 is:2.304821014404297\n",
      "The pseudo loss for epoch14 is:nan\n",
      "The supervised loss for epoch15 is:2.3053011894226074\n",
      "The pseudo loss for epoch15 is:nan\n",
      "The supervised loss for epoch16 is:2.3058533668518066\n",
      "The pseudo loss for epoch16 is:nan\n",
      "The supervised loss for epoch17 is:2.3046483993530273\n",
      "The pseudo loss for epoch17 is:nan\n",
      "The supervised loss for epoch18 is:2.3037657737731934\n",
      "The pseudo loss for epoch18 is:nan\n",
      "The supervised loss for epoch19 is:2.3045074939727783\n",
      "The pseudo loss for epoch19 is:nan\n",
      "The supervised loss for epoch20 is:2.3049991130828857\n",
      "The pseudo loss for epoch20 is:nan\n",
      "The supervised loss for epoch21 is:2.3027071952819824\n",
      "The pseudo loss for epoch21 is:nan\n",
      "The supervised loss for epoch22 is:2.3071558475494385\n",
      "The pseudo loss for epoch22 is:nan\n",
      "The supervised loss for epoch23 is:2.3058691024780273\n",
      "The pseudo loss for epoch23 is:nan\n",
      "The supervised loss for epoch24 is:2.305016040802002\n",
      "The pseudo loss for epoch24 is:nan\n",
      "The supervised loss for epoch25 is:2.303760290145874\n",
      "The pseudo loss for epoch25 is:nan\n",
      "The supervised loss for epoch26 is:2.3052966594696045\n",
      "The pseudo loss for epoch26 is:nan\n",
      "The supervised loss for epoch27 is:2.3059334754943848\n",
      "The pseudo loss for epoch27 is:nan\n",
      "The supervised loss for epoch28 is:2.3038833141326904\n",
      "The pseudo loss for epoch28 is:nan\n",
      "The supervised loss for epoch29 is:2.3039512634277344\n",
      "The pseudo loss for epoch29 is:nan\n",
      "The supervised loss for epoch30 is:2.304431438446045\n",
      "The pseudo loss for epoch30 is:nan\n",
      "The supervised loss for epoch31 is:2.306440591812134\n",
      "The pseudo loss for epoch31 is:nan\n",
      "The supervised loss for epoch32 is:2.303889751434326\n",
      "The pseudo loss for epoch32 is:nan\n",
      "The supervised loss for epoch33 is:2.305222988128662\n",
      "The pseudo loss for epoch33 is:nan\n",
      "The supervised loss for epoch34 is:2.3050270080566406\n",
      "The pseudo loss for epoch34 is:nan\n",
      "The supervised loss for epoch35 is:2.3043885231018066\n",
      "The pseudo loss for epoch35 is:nan\n",
      "The supervised loss for epoch36 is:2.3052029609680176\n",
      "The pseudo loss for epoch36 is:nan\n",
      "The supervised loss for epoch37 is:2.3078739643096924\n",
      "The pseudo loss for epoch37 is:nan\n",
      "The supervised loss for epoch38 is:2.3057804107666016\n",
      "The pseudo loss for epoch38 is:nan\n",
      "The supervised loss for epoch39 is:2.3060951232910156\n",
      "The pseudo loss for epoch39 is:nan\n",
      "The supervised loss for epoch40 is:2.3064358234405518\n",
      "The pseudo loss for epoch40 is:nan\n",
      "The supervised loss for epoch41 is:2.304006814956665\n",
      "The pseudo loss for epoch41 is:nan\n",
      "The supervised loss for epoch42 is:2.304474353790283\n",
      "The pseudo loss for epoch42 is:nan\n",
      "The supervised loss for epoch43 is:2.3054823875427246\n",
      "The pseudo loss for epoch43 is:nan\n",
      "The supervised loss for epoch44 is:2.306678295135498\n",
      "The pseudo loss for epoch44 is:nan\n",
      "The supervised loss for epoch45 is:2.30315899848938\n",
      "The pseudo loss for epoch45 is:nan\n",
      "The supervised loss for epoch46 is:2.3041858673095703\n",
      "The pseudo loss for epoch46 is:nan\n",
      "The supervised loss for epoch47 is:2.305640697479248\n",
      "The pseudo loss for epoch47 is:nan\n",
      "The supervised loss for epoch48 is:2.308840274810791\n",
      "The pseudo loss for epoch48 is:nan\n",
      "The supervised loss for epoch49 is:2.3053348064422607\n",
      "The pseudo loss for epoch49 is:nan\n",
      "The supervised loss for epoch50 is:2.3044545650482178\n",
      "The pseudo loss for epoch50 is:nan\n",
      "The supervised loss for epoch51 is:2.304325580596924\n",
      "The pseudo loss for epoch51 is:nan\n",
      "The supervised loss for epoch52 is:2.304643392562866\n",
      "The pseudo loss for epoch52 is:nan\n",
      "The supervised loss for epoch53 is:2.305508852005005\n",
      "The pseudo loss for epoch53 is:nan\n",
      "The supervised loss for epoch54 is:2.303534984588623\n",
      "The pseudo loss for epoch54 is:nan\n",
      "The supervised loss for epoch55 is:2.304842472076416\n",
      "The pseudo loss for epoch55 is:nan\n",
      "The supervised loss for epoch56 is:2.3081161975860596\n",
      "The pseudo loss for epoch56 is:nan\n",
      "The supervised loss for epoch57 is:2.306673526763916\n",
      "The pseudo loss for epoch57 is:nan\n",
      "The supervised loss for epoch58 is:2.3064558506011963\n",
      "The pseudo loss for epoch58 is:nan\n",
      "The supervised loss for epoch59 is:2.304802417755127\n",
      "The pseudo loss for epoch59 is:nan\n",
      "The supervised loss for epoch60 is:2.3049378395080566\n",
      "The pseudo loss for epoch60 is:nan\n",
      "The supervised loss for epoch61 is:2.304612159729004\n",
      "The pseudo loss for epoch61 is:nan\n",
      "The supervised loss for epoch62 is:2.305219888687134\n",
      "The pseudo loss for epoch62 is:nan\n",
      "The supervised loss for epoch63 is:2.3047609329223633\n",
      "The pseudo loss for epoch63 is:nan\n",
      "The supervised loss for epoch64 is:2.3050742149353027\n",
      "The pseudo loss for epoch64 is:nan\n",
      "The supervised loss for epoch65 is:2.3049333095550537\n",
      "The pseudo loss for epoch65 is:nan\n",
      "The supervised loss for epoch66 is:2.305022954940796\n",
      "The pseudo loss for epoch66 is:nan\n",
      "The supervised loss for epoch67 is:2.3059258460998535\n",
      "The pseudo loss for epoch67 is:nan\n",
      "The supervised loss for epoch68 is:2.306246280670166\n",
      "The pseudo loss for epoch68 is:nan\n",
      "The supervised loss for epoch69 is:2.3055975437164307\n",
      "The pseudo loss for epoch69 is:nan\n",
      "The supervised loss for epoch70 is:2.30436372756958\n",
      "The pseudo loss for epoch70 is:nan\n",
      "The supervised loss for epoch71 is:2.3049139976501465\n",
      "The pseudo loss for epoch71 is:nan\n",
      "The supervised loss for epoch72 is:2.309774398803711\n",
      "The pseudo loss for epoch72 is:nan\n",
      "The supervised loss for epoch73 is:2.304377317428589\n",
      "The pseudo loss for epoch73 is:nan\n",
      "The supervised loss for epoch74 is:2.3037519454956055\n",
      "The pseudo loss for epoch74 is:nan\n",
      "The supervised loss for epoch75 is:2.304255247116089\n",
      "The pseudo loss for epoch75 is:nan\n",
      "The supervised loss for epoch76 is:2.306591510772705\n",
      "The pseudo loss for epoch76 is:nan\n",
      "The supervised loss for epoch77 is:2.306267023086548\n",
      "The pseudo loss for epoch77 is:nan\n",
      "The supervised loss for epoch78 is:2.303971529006958\n",
      "The pseudo loss for epoch78 is:nan\n",
      "The supervised loss for epoch79 is:2.305634021759033\n",
      "The pseudo loss for epoch79 is:nan\n",
      "The supervised loss for epoch80 is:2.305280923843384\n",
      "The pseudo loss for epoch80 is:nan\n",
      "The supervised loss for epoch81 is:2.3050713539123535\n",
      "The pseudo loss for epoch81 is:nan\n",
      "The supervised loss for epoch82 is:2.3072071075439453\n",
      "The pseudo loss for epoch82 is:nan\n",
      "The supervised loss for epoch83 is:2.3049793243408203\n",
      "The pseudo loss for epoch83 is:nan\n",
      "The supervised loss for epoch84 is:2.303804397583008\n",
      "The pseudo loss for epoch84 is:nan\n",
      "The supervised loss for epoch85 is:2.3042569160461426\n",
      "The pseudo loss for epoch85 is:nan\n",
      "The supervised loss for epoch86 is:2.3067617416381836\n",
      "The pseudo loss for epoch86 is:nan\n",
      "The supervised loss for epoch87 is:2.305826187133789\n",
      "The pseudo loss for epoch87 is:nan\n",
      "The supervised loss for epoch88 is:2.30501389503479\n",
      "The pseudo loss for epoch88 is:nan\n",
      "The supervised loss for epoch89 is:2.3076565265655518\n",
      "The pseudo loss for epoch89 is:nan\n",
      "The supervised loss for epoch90 is:2.3064780235290527\n",
      "The pseudo loss for epoch90 is:nan\n",
      "The supervised loss for epoch91 is:2.3058955669403076\n",
      "The pseudo loss for epoch91 is:nan\n",
      "The supervised loss for epoch92 is:2.3049521446228027\n",
      "The pseudo loss for epoch92 is:nan\n",
      "The supervised loss for epoch93 is:2.306044816970825\n",
      "The pseudo loss for epoch93 is:nan\n",
      "The supervised loss for epoch94 is:2.305309772491455\n",
      "The pseudo loss for epoch94 is:nan\n",
      "The supervised loss for epoch95 is:2.3045296669006348\n",
      "The pseudo loss for epoch95 is:nan\n",
      "The supervised loss for epoch96 is:2.3053369522094727\n",
      "The pseudo loss for epoch96 is:nan\n",
      "The supervised loss for epoch97 is:2.304377317428589\n",
      "The pseudo loss for epoch97 is:nan\n",
      "The supervised loss for epoch98 is:2.304335832595825\n",
      "The pseudo loss for epoch98 is:nan\n",
      "The supervised loss for epoch99 is:2.304861068725586\n",
      "The pseudo loss for epoch99 is:nan\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=0.03)\n",
    "optimizer2 = optim.SGD(model1.parameters(), lr = 0.001)\n",
    "num_epochs = 100\n",
    "loss = 0\n",
    "unsupervised_loss = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    \n",
    "    # Train on labeled data\n",
    "    for images, labels in labeledLoader:\n",
    "        optimizer1.zero_grad()\n",
    "        outputs = model1(images)\n",
    "        supervised_loss = F.cross_entropy(outputs, labels)\n",
    "        supervised_loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "    print(f\"The supervised loss for epoch{epoch} is:{supervised_loss}\")\n",
    "\n",
    "    pseudo_images, pseudo_labels = pseudo_label(model1, unlabeledLoader)\n",
    "    \n",
    "    optimizer2.zero_grad()\n",
    "    outputs = model1(pseudo_images)                    \n",
    "    pseudo_loss = F.nll_loss(outputs, pseudo_labels) \n",
    "    pseudo_loss.backward()\n",
    "    optimizer2.step()   \n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"The pseudo loss for epoch{epoch} is:{pseudo_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 10.0%\n",
      "Testing Accuracy: 9.818181818181818%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(loader,model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  \n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "train_accuracy = calculate_accuracy(labeledLoader, model1)\n",
    "test_accuracy = calculate_accuracy(unlabeledLoader, model1)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}%')\n",
    "print(f'Testing Accuracy: {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy minimization for Two Moons Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_samples = np.load('./selected_samples.npy')\n",
    "\n",
    "remaining_samples = np.load('./remaining_dataset.npy')\n",
    "\n",
    "# Converting the selected samples and remaining samples into PyTorch tensors\n",
    "selected_samples_tensor = torch.tensor(selected_samples, dtype=torch.float32)\n",
    "remaining_samples_tensor = torch.tensor(remaining_samples, dtype=torch.float32)\n",
    "\n",
    "# Extracting features and labels for both datasets\n",
    "features_selected = selected_samples_tensor[:, :2]\n",
    "labels_selected = selected_samples_tensor[:, 2].long()  # converting labels to long for classification\n",
    "\n",
    "features_remaining = remaining_samples_tensor[:, :2]\n",
    "labels_remaining = remaining_samples_tensor[:, 2].long()\n",
    "\n",
    "# Creating TensorDatasets\n",
    "selected_dataset = TensorDataset(features_selected, labels_selected)\n",
    "remaining_dataset = TensorDataset(features_remaining, labels_remaining)\n",
    "\n",
    "# Creating DataLoaders\n",
    "selected_loader = DataLoader(selected_dataset, batch_size=1)  # small batch size for the small dataset\n",
    "remaining_loader = DataLoader(remaining_dataset, batch_size=10)  # larger batch size for the larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoMoonsNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(TwoMoonsNet,self).__init__()\n",
    "\n",
    "        self.fullyConnectedLayer = nn.Sequential(\n",
    "            nn.Linear(2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10,2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        output = self.fullyConnectedLayer(input)\n",
    "        activatedOutput = F.log_softmax(output, dim = 1)\n",
    "\n",
    "        return activatedOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = TwoMoonsNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The supervised loss for epoch0 is:0.48841455578804016\n",
      "The un-supervised loss for epoch0 is:0.5801041126251221\n",
      "The supervised loss for epoch1 is:0.42923659086227417\n",
      "The un-supervised loss for epoch1 is:0.5446867942810059\n",
      "The supervised loss for epoch2 is:0.38170090317726135\n",
      "The un-supervised loss for epoch2 is:0.5164116621017456\n",
      "The supervised loss for epoch3 is:0.34158214926719666\n",
      "The un-supervised loss for epoch3 is:0.4931500256061554\n",
      "The supervised loss for epoch4 is:0.3071553409099579\n",
      "The un-supervised loss for epoch4 is:0.4764765202999115\n",
      "The supervised loss for epoch5 is:0.2772395610809326\n",
      "The un-supervised loss for epoch5 is:0.46371200680732727\n",
      "The supervised loss for epoch6 is:0.2506945729255676\n",
      "The un-supervised loss for epoch6 is:0.45309799909591675\n",
      "The supervised loss for epoch7 is:0.22713176906108856\n",
      "The un-supervised loss for epoch7 is:0.4448125660419464\n",
      "The supervised loss for epoch8 is:0.20651240646839142\n",
      "The un-supervised loss for epoch8 is:0.4384482800960541\n",
      "The supervised loss for epoch9 is:0.18821103870868683\n",
      "The un-supervised loss for epoch9 is:0.4335406422615051\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer1 = optim.SGD(model2.parameters(), lr=0.03)\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr = 0.01)\n",
    "num_epochs = 10\n",
    "loss = 0\n",
    "unsupervised_loss = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model2.train()\n",
    "    \n",
    "    # Train on labeled data\n",
    "    for images, labels in selected_loader:\n",
    "        optimizer1.zero_grad()\n",
    "        outputs = model2(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "    print(f\"The supervised loss for epoch{epoch} is:{loss}\")\n",
    "    \n",
    "    # Train on unlabeled data\n",
    "    for images, labels in remaining_loader:\n",
    "        optimizer2.zero_grad()\n",
    "        outputs = model2(images)\n",
    "        unsupervised_loss = entropy_loss(outputs)\n",
    "        unsupervised_loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "    \n",
    "    print(f\"The un-supervised loss for epoch{epoch} is:{unsupervised_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.0%\n",
      "Testing Accuracy: 82.97872340425532%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calculate_accuracy(selected_loader, model2)\n",
    "test_accuracy = calculate_accuracy(remaining_loader, model2)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}%')\n",
    "print(f'Testing Accuracy: {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
